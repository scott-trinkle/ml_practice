{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this notebook, we take the features we created in \"Features.ipynb\" and use them to train a series of models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving models\n",
    "import joblib\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model building tools\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, RandomizedSearchCV, GridSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier#,StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Loading the whole dataset processed in \"Features.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/df_full.csv',index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-splitting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mask\n",
    "tr = ~df['Survived'].isnull()\n",
    "\n",
    "# Training data\n",
    "y = df[tr]['Survived'].to_numpy().astype(int)\n",
    "X = df[tr].drop('Survived',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting setup\n",
    "\n",
    "First, we define a series of data-scaling and feature selection pipelines to search over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to scale numerical columns, the rest have already\n",
    "# been one-hot encoded\n",
    "num_cols = ['Age','SibSp','Parch','Pclass','TicketFreq','FareBin']\n",
    "\n",
    "# Methods for scaling numerical columns\n",
    "\n",
    "minmax_sc = ColumnTransformer(transformers=[('num', MinMaxScaler(), num_cols)],remainder='passthrough')\n",
    "standard_sc = ColumnTransformer(transformers=[('num', StandardScaler(), num_cols)],remainder='passthrough')\n",
    "scalers = [None, minmax_sc, standard_sc]\n",
    "\n",
    "# Methods for feature selection/transformation\n",
    "feature_transforms = [None,  # original features\n",
    "                      SelectPercentile(percentile=75),  # keeps highest % based on univariate statistical tests\n",
    "                      PCA(n_components=round(X.shape[1]*0.75)),  # first 75% PCA components\n",
    "                      PolynomialFeatures(degree=2,interaction_only=True),  # 2-deg polynomial interaction terms\n",
    "                      PolynomialFeatures(degree=3,interaction_only=True)]  # 3-deg polynomial interaction terms\n",
    "\n",
    "# Default CV scheme\n",
    "cross_validator = StratifiedShuffleSplit(n_splits=10, train_size=0.8, random_state=49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some utility functions for performing the CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(best_model_pipeline, X,y):\n",
    "    '''\n",
    "    Takes in a cross-validated model and prints the best parameters used for the\n",
    "    data processing, feature engineering, and classifier steps. Also prints out\n",
    "    the training and average cross-validation accuracy. \n",
    "    '''\n",
    "    print(\"#------------ Best Data Pipeline found in RandomSearchCV  ------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n",
    "    print(\"\\n\\n#------------ Best Feature Engineering technique found in RandomSearchCV  ------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n",
    "    print(\"\\n\\n#------------ Best Classifier found in RandomSearchCV  ------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n",
    "    print(\"\\n\\n#------------ Best Estimator's Accuracy Score on training set ------------#\\n\\n\", best_model_pipeline.score(X,y))\n",
    "    print(\"\\n\\n#------------ Best Estimator's average Accuracy Score on CV (validation set) -------------#\\n\\n\", best_model_pipeline.best_score_)\n",
    "    \n",
    "def my_CV(clf_params, scalers = scalers, features = feature_transforms,\n",
    "         cv = None,n_iter=50,n_jobs=32,verbose=False, search_func=RandomizedSearchCV):\n",
    "    '''\n",
    "    Creates an optimal classification pipeline using RandomSearchCV. \n",
    "    \n",
    "    Data are first scaled by a column transformer in \"scalers,\" then feature selection\n",
    "    is performed by a method in \"features,\" followed by classification using a \n",
    "    classifier defined in clf_params. The parameter space of the classifier is also\n",
    "    defined in clf_params, and the entire pipeline is optimized using RandomSearchCV. \n",
    "    \n",
    "    Chosen parameters and results are printed at the end. \n",
    "    '''\n",
    "    \n",
    "    # Initializing default pipeline with a scaler step, feature transform step\n",
    "    # and classifier. These specific values are just placeholders that will \n",
    "    # replaced with values in params_grid\n",
    "    pipe = Pipeline(steps = [\n",
    "    ('scaler', standard_sc),\n",
    "    ('feature_transf', PCA()), \n",
    "    ('clf', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    # Parameter space to search over\n",
    "    params_grid = {\n",
    "     'scaler' : scalers,\n",
    "     'feature_transf': features\n",
    "    }\n",
    "    params_grid.update(clf_params)\n",
    "    \n",
    "    # Default CV scheme\n",
    "    if cv is None:\n",
    "        cv = StratifiedShuffleSplit(n_splits=10, train_size=0.8, random_state=49)\n",
    "        \n",
    "    if n_iter is None:\n",
    "        kwargs = {'param_grid' : params_grid}\n",
    "    else:\n",
    "        kwargs = {'param_distributions' : params_grid,\n",
    "                  'n_iter' : n_iter,\n",
    "                  'random_state' : 7}   \n",
    "    \n",
    "    # Search parameters and choose the pipeline with the highest CV accuracy\n",
    "    best_model_pipeline = search_func(estimator=pipe, \n",
    "                                             scoring='accuracy',\n",
    "                                             refit='accuracy', \n",
    "                                             n_jobs=n_jobs,\n",
    "                                             cv=cv, \n",
    "                                             verbose=verbose,\n",
    "                                             **kwargs)\n",
    "    best_model_pipeline.fit(X, y)\n",
    "    print_results(best_model_pipeline,X,y)\n",
    "    \n",
    "    return best_model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Here, we fit the actual models using the utility functions we built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------ Best Data Pipeline found in RandomSearchCV  ------------#\n",
      "\n",
      " ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
      "                  transformer_weights=None,\n",
      "                  transformers=[('num',\n",
      "                                 StandardScaler(copy=True, with_mean=True,\n",
      "                                                with_std=True),\n",
      "                                 ['Age', 'SibSp', 'Parch', 'Pclass',\n",
      "                                  'TicketFreq', 'FareBin'])],\n",
      "                  verbose=False)\n",
      "\n",
      "\n",
      "#------------ Best Feature Engineering technique found in RandomSearchCV  ------------#\n",
      "\n",
      " PolynomialFeatures(degree=2, include_bias=True, interaction_only=True,\n",
      "                   order='C')\n",
      "\n",
      "\n",
      "#------------ Best Classifier found in RandomSearchCV  ------------#\n",
      "\n",
      " LogisticRegression(C=0.4344110413285817, class_weight=None, dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "\n",
      "\n",
      "#------------ Best Estimator's Accuracy Score on training set ------------#\n",
      "\n",
      " 0.8900112233445566\n",
      "\n",
      "\n",
      "#------------ Best Estimator's average Accuracy Score on CV (validation set) -------------#\n",
      "\n",
      " 0.8569832402234637\n"
     ]
    }
   ],
   "source": [
    "logreg_pipe = my_CV(\n",
    "    clf_params = {\n",
    "        'clf' : [LogisticRegression(solver='saga',penalty='l1'),  # Need to specify different solver for l1 vs l2\n",
    "                 LogisticRegression(solver='lbfgs',penalty='l2')],\n",
    "        'clf__C' : stats.loguniform(0.005, 1)\n",
    "    },\n",
    "    n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "joblib.dump(logreg_pipe.best_estimator_,'models/best_logreg.joblib');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------ Best Data Pipeline found in RandomSearchCV  ------------#\n",
      "\n",
      " ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
      "                  transformer_weights=None,\n",
      "                  transformers=[('num',\n",
      "                                 MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
      "                                 ['Age', 'SibSp', 'Parch', 'Pclass',\n",
      "                                  'TicketFreq', 'FareBin'])],\n",
      "                  verbose=False)\n",
      "\n",
      "\n",
      "#------------ Best Feature Engineering technique found in RandomSearchCV  ------------#\n",
      "\n",
      " PCA(copy=True, iterated_power='auto', n_components=19, random_state=None,\n",
      "    svd_solver='auto', tol=0.0, whiten=False)\n",
      "\n",
      "\n",
      "#------------ Best Classifier found in RandomSearchCV  ------------#\n",
      "\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=31, p=2,\n",
      "                     weights='distance')\n",
      "\n",
      "\n",
      "#------------ Best Estimator's Accuracy Score on training set ------------#\n",
      "\n",
      " 0.978675645342312\n",
      "\n",
      "\n",
      "#------------ Best Estimator's average Accuracy Score on CV (validation set) -------------#\n",
      "\n",
      " 0.8357541899441341\n"
     ]
    }
   ],
   "source": [
    "knn_pipe = my_CV(\n",
    "    clf_params = {\n",
    "        'clf' : [KNeighborsClassifier()],\n",
    "        'clf__n_neighbors' : stats.randint(1,50),\n",
    "        'clf__weights' : ['uniform','distance'],\n",
    "        'clf__metric' : ['minkowski','euclidean']\n",
    "    },\n",
    "    n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "joblib.dump(knn_pipe.best_estimator_,'models/best_knn.joblib');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------ Best Data Pipeline found in RandomSearchCV  ------------#\n",
      "\n",
      " ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
      "                  transformer_weights=None,\n",
      "                  transformers=[('num',\n",
      "                                 StandardScaler(copy=True, with_mean=True,\n",
      "                                                with_std=True),\n",
      "                                 ['Age', 'SibSp', 'Parch', 'Pclass',\n",
      "                                  'TicketFreq', 'FareBin'])],\n",
      "                  verbose=False)\n",
      "\n",
      "\n",
      "#------------ Best Feature Engineering technique found in RandomSearchCV  ------------#\n",
      "\n",
      " PolynomialFeatures(degree=2, include_bias=True, interaction_only=True,\n",
      "                   order='C')\n",
      "\n",
      "\n",
      "#------------ Best Classifier found in RandomSearchCV  ------------#\n",
      "\n",
      " SVC(C=0.012781635090469897, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "\n",
      "#------------ Best Estimator's Accuracy Score on training set ------------#\n",
      "\n",
      " 0.8529741863075196\n",
      "\n",
      "\n",
      "#------------ Best Estimator's average Accuracy Score on CV (validation set) -------------#\n",
      "\n",
      " 0.852513966480447\n"
     ]
    }
   ],
   "source": [
    "svc_pipe = my_CV(\n",
    "    clf_params = {\n",
    "        'clf' : [SVC(probability=True)],\n",
    "        'clf__C' : stats.loguniform(0.01,10),\n",
    "        'clf__kernel' : ['linear','poly','rbf']\n",
    "    },\n",
    "    n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "joblib.dump(svc_pipe.best_estimator_,'models/best_svc.joblib');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------ Best Data Pipeline found in RandomSearchCV  ------------#\n",
      "\n",
      " ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
      "                  transformer_weights=None,\n",
      "                  transformers=[('num',\n",
      "                                 StandardScaler(copy=True, with_mean=True,\n",
      "                                                with_std=True),\n",
      "                                 ['Age', 'SibSp', 'Parch', 'Pclass',\n",
      "                                  'TicketFreq', 'FareBin'])],\n",
      "                  verbose=False)\n",
      "\n",
      "\n",
      "#------------ Best Feature Engineering technique found in RandomSearchCV  ------------#\n",
      "\n",
      " None\n",
      "\n",
      "\n",
      "#------------ Best Classifier found in RandomSearchCV  ------------#\n",
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=58,\n",
      "                       n_jobs=None, oob_score=False, random_state=7, verbose=0,\n",
      "                       warm_start=False)\n",
      "\n",
      "\n",
      "#------------ Best Estimator's Accuracy Score on training set ------------#\n",
      "\n",
      " 0.8754208754208754\n",
      "\n",
      "\n",
      "#------------ Best Estimator's average Accuracy Score on CV (validation set) -------------#\n",
      "\n",
      " 0.8564245810055866\n"
     ]
    }
   ],
   "source": [
    "rf_pipe = my_CV(\n",
    "    clf_params = {\n",
    "        'clf' : [RandomForestClassifier(random_state=7)],\n",
    "        'clf__n_estimators': stats.randint(10, 175),\n",
    "        'clf__criterion' : ['gini', 'entropy'],\n",
    "        'clf__max_features': [None, \"auto\", \"log2\"],\n",
    "        'clf__max_depth': list(np.arange(1,6,dtype=int)) + [None]\n",
    "    },\n",
    "    n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "joblib.dump(rf_pipe.best_estimator_,'models/best_rf.joblib');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------ Best Data Pipeline found in RandomSearchCV  ------------#\n",
      "\n",
      " ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
      "                  transformer_weights=None,\n",
      "                  transformers=[('num',\n",
      "                                 StandardScaler(copy=True, with_mean=True,\n",
      "                                                with_std=True),\n",
      "                                 ['Age', 'SibSp', 'Parch', 'Pclass',\n",
      "                                  'TicketFreq', 'FareBin'])],\n",
      "                  verbose=False)\n",
      "\n",
      "\n",
      "#------------ Best Feature Engineering technique found in RandomSearchCV  ------------#\n",
      "\n",
      " None\n",
      "\n",
      "\n",
      "#------------ Best Classifier found in RandomSearchCV  ------------#\n",
      "\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, eta=0.1008355757194972,\n",
      "              gamma=0.7001745261557822, gpu_id=-1, importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.100835577,\n",
      "              max_delta_step=0, max_depth=2, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=74, n_jobs=0,\n",
      "              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "\n",
      "\n",
      "#------------ Best Estimator's Accuracy Score on training set ------------#\n",
      "\n",
      " 0.8597081930415263\n",
      "\n",
      "\n",
      "#------------ Best Estimator's average Accuracy Score on CV (validation set) -------------#\n",
      "\n",
      " 0.8547486033519553\n"
     ]
    }
   ],
   "source": [
    "xgb_pipe = my_CV(\n",
    "    clf_params = {\n",
    "        'clf' : [XGBClassifier()],\n",
    "        'clf__n_estimators': stats.randint(5, 125),\n",
    "     'clf__eta': stats.loguniform(0.01, 1),\n",
    "     'clf__max_depth': [None] + list(np.arange(1,7,dtype=int)),\n",
    "     'clf__gamma': stats.loguniform(0.01, 1)\n",
    "    },\n",
    "    n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "joblib.dump(xgb_pipe.best_estimator_,'models/best_xgb.joblib');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
